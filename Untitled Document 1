import json
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# Load the JSON dataset
with open('/home/user/Desktop/Poojan's/op.json', 'r') as f:
    data = json.load(f)

# Convert the data into a list of dictionaries suitable for Hugging Face datasets
formatted_data = [
    {
        "instruction": item["instruction"],
        "input": item["input"],
        "output": item["output"]["Description"]  # Choose a relevant field from the "output"
    }
    for item in data
]

# Create a Hugging Face Dataset
dataset = Dataset.from_dict(formatted_data)

# Initialize the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_function(examples):
    # Combine instruction and input for a BERT input
    inputs = examples['instruction'] + " " + examples['input']
    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128)
    
    # Tokenize the output
    labels = tokenizer(examples['output'], padding='max_length', truncation=True, max_length=128)
    
    # We use the input_ids for both input and labels
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize the dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Load the pretrained BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(tokenizer))

# Set up the training arguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=8,   # batch size for training
    per_device_eval_batch_size=8,    # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=tokenized_dataset,     # training dataset
    eval_dataset=tokenized_dataset       # evaluation dataset
)

# Fine-tune the model
trainer.train()

# Evaluate the model
trainer.evaluate()

# Save the fine-tuned model
model.save_pretrained('./my_finetuned_model')

